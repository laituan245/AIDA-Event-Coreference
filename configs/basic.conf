basic {
  # Data-Related Configs
  use_groundtruth = true

  # Model-Related Configs
  transformer = hfl/chinese-roberta-wwm-ext-large
  multi_piece_strategy = average
  ffnn_size = 1000
  ffnn_depth = 2
  feature_size = 50

  # Additional Features Configs
  use_event_type_features = false

  # Training/Inference Configs
  gradient_checkpointing = true
  transformer_learning_rate = 5e-05
  task_learning_rate = 0.0005
  epochs = 50
  batch_size = 16
  dropout_rate = 0.5
  transformer_dropout_rate = 0.5
  max_grad_norm = 1.0
  transformer_weight_decay = 0.1

  # Others
  no_cuda = false
}
